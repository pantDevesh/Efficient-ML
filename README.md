# Efficient-ML

### Papers on Efficient Training and Inference of LLMs and Other Deep Neural Networks

| **Title** | **Summary** |
|:--|:----:|
| [![Star](https://img.shields.io/github/stars/ruikangliu/FlatQuant.svg?style=social&label=Star)](https://github.com/ruikangliu/FlatQuant)<br>[FLATQUANT: Flatness Matters for LLM Quantization](https://huggingface.co/papers/2410.09426)<br>Published: 12/10/24<br>Accessed: 22/10/24<br>Conference: NA<br>Group: NA|<img width="1200" alt="image" src="figures/flat_quant.png"><br><p align="left">1. Flattens weight and activation distributions for better quantization.<br>2. Optimizes affine transformations per linear layer.<br>3. Uses
| [![Star](https://img.shields.io/github/stars/haiquanlu/AlphaPruning.svg?style=social&label=Star)](https://github.com/haiquanlu/AlphaPruning)<br>[AlphaPruning](https://arxiv.org/pdf/2410.10912)</br>Published: 02/10/24</br>Accessed: 24/10/24<br>Conference: Neurips<br>Group: NA|<img width="800" alt="image" src="figures/alpha_pruning.png"> </br> <p align="left"> 1. Suggests a layerwise purning of LLMs for faster infernence </br> 2. Uses HT-SR(Heavytailed Self-regularization) theory and ESDs(Empirical Spectral Density) of matrices to decide layerwise pruning ratios  .</br> 3. Shape metrics outperform scale metrics for computing importance of each layer for pruning.</br></p>
| [Relaxed Recursive Transformers](https://arxiv.org/abs/2410.20672)</br>Published: 28/10/24<br>Accessed: 12/11/24<br>Conference:ICLR(under reivew)<br>Group: Kaist AI/Deepmind <br>|<img width="1200" alt="image" src="figures/relaxed_recur_transformer.png"> <br> <p align="left"> 1. Introduces LoRA adapters for each layer in the Recursive Transformer, relaxing strict parameter sharing.<br>2. Finds that averaging the weight matrices of tied layers yields the best results.<br>3. Proposes continuous depth-wise batching, leveraging the recursive nature of the architecture to achieve 2-3 times faster inference.<br>4. Outperforms similarly sized pretrained LLMs by using a single block of unique layers and minimal fine-tuning on 15B tokens.</p>