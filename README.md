# Efficient-ML

### Papers on Efficient Training and Inference of LLMs and Other Deep Neural Networks

| **Title** | **Summary** |
|:--|:----:|
| [![Star](https://img.shields.io/github/stars/ruikangliu/FlatQuant.svg?style=social&label=Star)](https://github.com/ruikangliu/FlatQuant)<br>[FLATQUANT: Flatness Matters for LLM Quantization](https://huggingface.co/papers/2410.09426)</br>Published: 12/10/24</br>Accessed: 22/10/24<br>Conference: NA<br>Group: NA|<img width="1200" alt="image" src="figures/image.png"> </br> <p align="left"> 1. Flattens weight and activation distributions for better quantization. </br> 2. Optimizes affine transformations per linear layer.</br> 3. Uses lightweight, block-wise training on calibration data.</br>4. Uses Kronecker decomposition to reduce memory/compute overhead.</br></p>


